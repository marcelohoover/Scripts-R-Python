import pandas as pd
import pyspark
import pydeequ
import awswrangler as wr 
from pyspark.sql import SparkSession, SQLContext, Row
#from pyspark.sql.functions import col, lit
from pyspark.sql.types import IntegerType
from pydeequ.analyzers import *
from pydeequ.profiles import *
from pydeequ.checks import *
from pydeequ.verification import *
from pydeequ.suggestions import *
from pyspark.sql.functions import *

# Session
spark = (SparkSession
    .builder
    .config("spark.jars.packages", pydeequ.deequ_maven_coord)
    .config("spark.jars.excludes", pydeequ.f2j_maven_coord)
    .getOrCreate())

spark.sparkContext.setLogLevel("ERROR")    

# Conf pyspark
conf = pyspark.SparkConf()
sc = pyspark.SparkContext.getOrCreate(conf=conf)
sqlContext = SQLContext(sc)

# Carga S3 Beneficiarios
df_s3 = sqlContext.read.options(delimiter=',' ,header='True').csv(f's3://marcelo-pimentel-bucket/data_gov/poc_data-gov/input/beneficiarios/tb_Previsionales_Beneficiarios.csv')
#df_s3.select('sexo').distinct().show()


df_s3 = df_s3.withColumn("edad", df_s3["edad"].cast(IntegerType()))
df_s3 = df_s3.withColumn("numdependentes", df_s3["numdependentes"].cast(IntegerType()))

#df_s3.printSchema()


#Check Beneficiarios
#AnalysisResult -  distinct count and completeness for each column.
analysisResult = AnalysisRunner(spark) \
                    .onData(df_s3) \
                    .addAnalyzer(Size()) \
                    .addAnalyzer(Completeness("edad")) \
                    .addAnalyzer(ApproxCountDistinct("edad")) \
                    .addAnalyzer(Mean("edad")) \
                    .addAnalyzer(Completeness("numdependentes")) \
                    .addAnalyzer(ApproxCountDistinct("numdependentes")) \
                    .addAnalyzer(Mean("numdependentes")) \
                    .addAnalyzer(Correlation("edad", "numdependentes")) \
                    .addAnalyzer(Completeness("nombres")) \
                    .addAnalyzer(ApproxCountDistinct("nombres")) \
                    .addAnalyzer(Completeness("apellidomaterno")) \
                    .addAnalyzer(ApproxCountDistinct("apellidomaterno")) \
                    .addAnalyzer(Completeness("apellidopaterno")) \
                    .addAnalyzer(ApproxCountDistinct("apellidopaterno")) \
                    .addAnalyzer(Completeness("pensionpersona")) \
                    .addAnalyzer(ApproxCountDistinct("pensionpersona")) \
                    .addAnalyzer(Completeness("relacionhijomadre")) \
                    .addAnalyzer(ApproxCountDistinct("relacionhijomadre")) \
                    .addAnalyzer(Completeness("situacioninvalidez")) \
                    .addAnalyzer(ApproxCountDistinct("situacioninvalidez")) \
                    .addAnalyzer(Completeness("derechoacrecer")) \
                    .addAnalyzer(ApproxCountDistinct("derechoacrecer")) \
                    .addAnalyzer(Completeness("derechopension")) \
                    .addAnalyzer(ApproxCountDistinct("derechopension")) \
                    .addAnalyzer(Completeness("rutbeneficiario")) \
                    .addAnalyzer(ApproxCountDistinct("rutbeneficiario")) \
                    .addAnalyzer(Completeness("dvrutbeneficiario")) \
                    .addAnalyzer(ApproxCountDistinct("dvrutbeneficiario")) \
                    .addAnalyzer(Completeness("numpoliza")) \
                    .addAnalyzer(ApproxCountDistinct("numpoliza")) \
                    .addAnalyzer(Completeness("numorden")) \
                    .addAnalyzer(ApproxCountDistinct("numorden")) \
                    .addAnalyzer(Completeness("sexo")) \
                    .addAnalyzer(ApproxCountDistinct("sexo")) \
                    .addAnalyzer(Completeness("requisitopension")) \
                    .addAnalyzer(ApproxCountDistinct("requisitopension")) \
                    .addAnalyzer(Completeness("tipobeneficiario")) \
                    .addAnalyzer(ApproxCountDistinct("tipobeneficiario")) \
                    .addAnalyzer(Completeness("fechanacimiento")) \
                    .addAnalyzer(ApproxCountDistinct("fechanacimiento")) \
                    .addAnalyzer(Completeness("fechainvalidez")) \
                    .addAnalyzer(ApproxCountDistinct("fechainvalidez")) \
                    .addAnalyzer(Completeness("fechafallecimiento")) \
                    .addAnalyzer(ApproxCountDistinct("fechafallecimiento")) \
                    .addAnalyzer(Completeness("fechanacimientohijomenor")) \
                    .addAnalyzer(ApproxCountDistinct("fechanacimientohijomenor")) \
                    .run()

analysisResult_df_s3 = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)

#add el periodo de prueba del analyzer
dataframe_analysisResult =  analysisResult_df_s3.withColumn("periodo",current_timestamp())

dataframe_analysisResult = dataframe_analysisResult.toPandas()

# Resultados analizer en S3
#analysisResult_df_s3.coalesce(1).write.format('csv').option('header','true').mode("overwrite").save("s3a://marcelo-pimentel-bucket/data_gov/poc_data-gov/output/beneficiarios/")


# Etiquetas para la tabla beneficiarios AnalyzerResults
param = {
    "source": "Data Quality - Beneficiarios AnalyzerResults",
    "class": "DQ en Deequ"
}

# Etiqueta para las columnas de la tabla beneficiarios
comments = {
    "entity": "Tipo de objecto (tablas or columns)",
    "instance": "Nombre de la entidad",
    "name": "Nombre del analyzer (metodo)",
    "value": "Resultado del analyzer (percentaje/metrica)",
    "periodo": "Periodo del check de calidad"
}

res = wr.s3.to_parquet(
    df=dataframe_analysisResult,
    path=f"s3://marcelo-pimentel-bucket/data_gov/poc_data-gov/output/beneficiarios/",
    dataset=True,
    database="data-gov-poc",
    table="beneficiarios_Analyzer_Results", #La crea caso no exciste (de acuerdo con los grants de lake formation)
    mode="append", # apend / overwrite partition
    description="Tabla de beneficiarios - POC Data Governance Calidad de datos - Analyzer Results en PyDeequ",
    parameters=param,
    columns_comments = comments
)

#Check Beneficiarios
#VerificationSuite - Reglas de calidad aplicadas de acuerdo con el requierimiento del negocio.

check = Check(spark, CheckLevel.Error, "Data Quality rules - Review Check")

checkResult = VerificationSuite(spark) \
    .onData(df_s3) \
    .addCheck(
        check.hasSize(lambda x: x >= 30000) \
        .isComplete("rutbeneficiario")  \
        .isUnique("rutbeneficiario")  \
        .isNonNegative("rutbeneficiario") \
        .isUnique("nombres") \
        .isNonNegative("pensionpersona") \
        .isNonNegative("relacionhijomadre") \
        .isNonNegative("numpoliza") \
        .isNonNegative("numorden") \
        .isContainedIn("situacioninvalidez", ['N', 'T', 'P']) \
        .isContainedIn("derechoacrecer", ['N', 'S']) \
        .isContainedIn("derechopension", ['99', '10', '20']) \
        .isContainedIn("dvrutbeneficiario", ['0','1', '2', '3', '4', '5', '6', '7', '8', '9', 'K']) \
        .isContainedIn("requisitopension", ['1', '2', '3', '4', '5', '6', '7', '8', '9']) \
        .hasMin("edad", lambda x: x == 1.0) \
        .hasMax("edad", lambda x: x <= 100.0)  \
        .isContainedIn("sexo", ['M', 'F']) \
        .hasPattern(column = "sexo", pattern = r'^M?$|^F?$', assertion = lambda x : x >= 0.9) \
        .hasMin("numdependentes", lambda x: x == 0.0)  \
        .hasMax("numdependentes", lambda x: x == 10.0)  \
        .hasPattern("apellidomaterno", r'^[A-Za-z\s\#]+$', lambda x : x == 1.0 ) \
        .hasPattern("apellidopaterno", r'^[A-Za-z\s\#]+$', lambda x : x == 1.0) \
        .hasPattern("nombres", r'^[A-Za-z\s\#]+$', lambda x : x == 1.0) \
        .satisfies("fechanacimiento >= '1900-01-01'", "fechanacimiento", lambda x : x == 1.0) \
        .satisfies("fechainvalidez >= '1900-01-01' or fechainvalidez is null", "fechainvalidez", lambda x: x == 1.0) \
        .satisfies("fechafallecimiento >= '1900-01-01' or fechafallecimiento is null", "fechafallecimiento", lambda x: x == 1.0) \
        .satisfies("fechanacimientohijomenor >= '1900-01-01' or fechanacimientohijomenor is null", "fechanacimientohijomenor", lambda x: x == 1.0) \
        .isContainedIn("tipobeneficiario", ['99', '10', '11', '20', '21', '30', '35', '41', '42', '50', '51', '77'])) \
    .run()
verification_df_s3 = VerificationResult.checkResultsAsDataFrame(spark, checkResult)

#add el periodo de prueba del verification
dataframe_verificationResult =  verification_df_s3.withColumn("periodo",current_timestamp())

dataframe_verificationResult = dataframe_verificationResult.toPandas()

# Resultados verification en S3
#verification_df_s3.coalesce(1).write.format('csv').option('header','true').mode("overwrite").save("s3a://marcelo-pimentel-bucket/data_gov/poc_data-gov/output/beneficiarios/")


# Etiquetas para la tabla beneficiarios VerificationResults
param_verification = {
    "source": "Data Quality - Beneficiarios VerificationResults",
    "class": "DQ en Deequ"
}

# Etiqueta para las columnas de la tabla beneficiarios
comments_verification = {
    "check": "Tipo de Review",
    "check_level": "Level of Review",
    "check_status": "Status of Review",
    "constraint": "Prueba aplicada al tipo de dato",
    "constraint_status": "Resultado de la prueba",
    "constraint_message": "Percentaje de registros en complaince con la regla",
    "periodo": "Periodo de la prueba de calidad"
    
}

res = wr.s3.to_parquet(
    df=dataframe_verificationResult,
    path=f"s3://marcelo-pimentel-bucket/data_gov/poc_data-gov/output/beneficiarios/",
    dataset=True,
    database="data-gov-poc",
    table="beneficiarios_Verification_Results", #La crea caso no exciste (de acuerdo con los grants de lake formation)
    mode="append", # append / overwrite partition
    description="Tabla de beneficiarios - POC Data Governance Calidad de datos - Verification Results en PyDeequ",
    parameters=param_verification,
    columns_comments = comments_verification

)

# Close gateway or glue is never gonna finish 'till timeout
spark.sparkContext._gateway.close()
